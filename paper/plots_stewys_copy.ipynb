{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import yaml\n",
    "import tqdm\n",
    "\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 600\n",
    "# plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_qs = []\n",
    "for topic in glob.glob('../questions/topics/*'):\n",
    "    if 'Demo' not in topic:\n",
    "        with open(f\"{topic}/questions.yaml\") as f:\n",
    "            all_qs.extend(yaml.safe_load(f))\n",
    "\n",
    "all_qs = pd.DataFrame(all_qs)\n",
    "correct_answers = dict(zip(all_qs['Id'], all_qs['MultipleChoice'].apply(lambda r: r['Correct'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_data = []\n",
    "\n",
    "for file in glob.glob('../backend/users/*.yaml'):\n",
    "    if 'demo' not in file:\n",
    "        with open(file) as f:\n",
    "            data = yaml.safe_load(f)\n",
    "\n",
    "        if 'pretest' in data:\n",
    "            data['pretest'] = pd.DataFrame(data['pretest'])\n",
    "        if 'questionSchedule' not in data:\n",
    "            continue\n",
    "        \n",
    "        qSchedule = []\n",
    "        for i, day in enumerate(data['questionSchedule']):\n",
    "            for j, q in enumerate(day):\n",
    "                q['day'] = i\n",
    "                q['numInDay'] = j\n",
    "                qSchedule.append(q)\n",
    "        data['questionSchedule'] = pd.DataFrame(qSchedule)\n",
    "\n",
    "        if 'posttestA' in data:\n",
    "            data['posttestA'] = pd.DataFrame(data['posttestA'])\n",
    "        if 'posttestB' in data:\n",
    "            data['posttestB'] = pd.DataFrame(data['posttestB'])\n",
    "        if 'sleepData' in data:\n",
    "            data['sleepData'] = np.array([float(d['numHours']) for d in data['sleepData']])\n",
    "\n",
    "        all_user_data.append(data)\n",
    "\n",
    "all_user_data = pd.DataFrame(all_user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_emails = ['madison.evans@som.umaryland.edu', 'puja.patel@som.umaryland.edu', 'kran2@jh.edu', 'charles1@usf.edu']\n",
    "bad_emails = []\n",
    "finished_study_data = all_user_data[all_user_data['status'].isin(['posttestDone', 'studyDone', 'posttestPartADone']) & ~all_user_data['email'].isin(bad_emails)]\n",
    "finished_users = finished_study_data['email'].unique()\n",
    "print(f\"Participants who finished study portion: {len(finished_study_data)}\\n{finished_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_post_data = all_user_data[all_user_data['status'].isin(['posttestDone']) & ~all_user_data['email'].isin(bad_emails)].reset_index()\n",
    "print(f\"Participants who finished all post-tests: {len(finished_post_data)}\\n{finished_post_data['email'].to_list()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraction contested\n",
    "\n",
    "Calculating fraction contested overall and per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../backend/scoring/contestedEvaluations.yaml') as f:\n",
    "    contested_evaluations = pd.DataFrame(yaml.safe_load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_questions = 180\n",
    "\n",
    "frac_contested = len(contested_evaluations[contested_evaluations['user'].isin(finished_users)]) / (len(finished_users) * total_questions)\n",
    "print(f\"Percentage of overall responses contested: {100 * frac_contested:.2f}%\")\n",
    "\n",
    "frac_contested_per_user = {user: (contested_evaluations['user'] == user).sum() / total_questions for user in finished_users}\n",
    "plt.title('Percentage of responses contested per user')\n",
    "plt.bar(frac_contested_per_user.keys(), np.array(list(frac_contested_per_user.values())) * 100)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contested_evaluations['correct'] = [correct_answers[qid] for qid in contested_evaluations['QID']]\n",
    "contested_evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "\n",
    "document = docx.Document()\n",
    "questions = contested_evaluations['QID'].map(lambda qid: all_qs['Question'][all_qs['Id'] == qid].item())\n",
    "\n",
    "for i, qid in enumerate(contested_evaluations[\"QID\"]):\n",
    "    document.add_paragraph(f\"QID: {contested_evaluations['QID'][i]}\")\n",
    "\n",
    "    topic = ' '.join(qid.split(' ')[:-1])\n",
    "    figs = all_qs[\"Figures\"][all_qs[\"Id\"] == qid].item()\n",
    "    if isinstance(figs, list):\n",
    "        for f in figs:\n",
    "            document.add_picture(f\"../questions/topics/{topic}/{f}\", height=docx.shared.Inches(2))\n",
    "\n",
    "    document.add_paragraph(f\"Question: {questions[i]}\")\n",
    "    document.add_paragraph(f\"User Response: {contested_evaluations['userResponse'][i]}\")\n",
    "    document.add_paragraph(f\"Correct Response: {contested_evaluations['correct'][i]}\")\n",
    "    document.add_paragraph(\"Was User Correct?: \")\n",
    "    document.add_paragraph(\"Comments: \")\n",
    "    document.add_paragraph(f\"SAIL Score: {contested_evaluations['score'][i]}\")\n",
    "    document.add_paragraph(f\"User: {contested_evaluations['user'][i]}\")\n",
    "    document.add_page_break()\n",
    "\n",
    "document.save(\"contested_evaluations.docx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability of voice transcription\n",
    "\n",
    "Calculating fraction of times they edited voice transcription, overall and per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_voice_responses = 60\n",
    "voice_edited_responses = finished_study_data['questionSchedule'].apply(\n",
    "    lambda qs: qs[(qs['modality'] == 'voice') & (qs['userResponse'] != qs['originalResponse'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_edited_per_user = voice_edited_responses.apply(len) / num_voice_responses\n",
    "\n",
    "print(f\"Percentage of overall responses edited: {100 * frac_edited_per_user.mean():.2f}%\")\n",
    "\n",
    "plt.title('Percentage of responses edited per user')\n",
    "plt.bar(finished_study_data['email'], frac_edited_per_user * 100)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study example response edits\n",
    "voice_edited_responses[finished_study_data['email'] == 'andrewbharris@jhmi.edu'].squeeze()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study Performance per Modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_study_accuracy(qs, modality=None):\n",
    "    if modality is not None:\n",
    "        qs = qs[qs['modality'] == modality]\n",
    "    return qs['score'].mean()\n",
    "\n",
    "overall = finished_study_data['questionSchedule'].apply(lambda qs: calc_study_accuracy(qs)).mean()\n",
    "voice = finished_study_data['questionSchedule'].apply(lambda qs: calc_study_accuracy(qs, 'voice')).mean()\n",
    "voiceless = finished_study_data['questionSchedule'].apply(lambda qs: calc_study_accuracy(qs, 'voiceless')).mean()\n",
    "mc = finished_study_data['questionSchedule'].apply(lambda qs: calc_study_accuracy(qs, 'mc')).mean()\n",
    "\n",
    "sems = [finished_study_data['questionSchedule'].apply(lambda qs: calc_study_accuracy(qs, modality)).sem() for modality in [None, \"voice\", \"voiceless\", \"mc\"]]\n",
    "\n",
    "plt.title('Accuracy during study per modality')\n",
    "plt.bar(['Overall', 'Voice', 'Voiceless', 'MC'], [overall, voice, voiceless, mc], yerr=sems, alpha=1, ecolor='black', capsize=10)\n",
    "plt.plot()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement per Modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall1 = pd.DataFrame(finished_post_data.iloc|[0][\"first_posttest\"][\"A\"])[\"automated_recall_score\"]\n",
    "recall2 = pd.DataFrame(finished_post_data.iloc[1][\"first_posttest\"][\"A\"])[\"automated_recall_score\"]\n",
    "recall1.to_numpy() == recall2.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_test_accuracies(test_type, modality, test_method):\n",
    "    \n",
    "    accs = []\n",
    "    for _, row in finished_post_data.iterrows():\n",
    "        qSched = row['questionSchedule']\n",
    "        if modality == 'all':\n",
    "            modality_qids = qSched['qid'].unique()\n",
    "        else:\n",
    "            modality_qids = qSched['qid'][qSched['modality'] == modality].unique()\n",
    "\n",
    "        test = row[test_type.split(\".\")[0]]\n",
    "        if \".\" in test_type:\n",
    "            test = test[test_type.split(\".\")[1]]\n",
    "        \n",
    "        test = pd.DataFrame(test)\n",
    "        if test_method == 'recognition':\n",
    "            accuracy = np.mean([test['response'][i] == correct_answers[test['QID'][i]]\n",
    "                                for i in range(len(test)) if test['QID'][i] in modality_qids])\n",
    "        elif test_method == 'recall':\n",
    "            accuracy = np.mean([test['automated_recall_score'][i]\n",
    "                                for i in range(len(test)) if test['QID'][i] in modality_qids])\n",
    "        \n",
    "        accs.append(accuracy)\n",
    "    \n",
    "    return accs\n",
    "\n",
    "pretest_acc = calc_test_accuracies('pretest', 'all', test_method=\"recognition\")\n",
    "posttest_recall_acc = []\n",
    "posttest_recognition_acc = []\n",
    "\n",
    "for posttest_iteration in ['first_posttest', 'second_posttest', 'posttest']:\n",
    "    recall_testname = f'{posttest_iteration}.A'\n",
    "    recog_testname = f'{posttest_iteration}.B'\n",
    "    if posttest_iteration == \"posttest\":\n",
    "        recall_testname = \"posttestA\"\n",
    "        recog_testname = \"posttestB\"\n",
    "    \n",
    "    posttest_recall_acc.append({modality: calc_test_accuracies(recall_testname, modality, test_method='recall')\n",
    "                                for modality in ['all', 'voice', 'voiceless', 'mc']})\n",
    "    posttest_recognition_acc.append({modality: calc_test_accuracies(recog_testname, modality, test_method=\"recognition\")\n",
    "                                     for modality in ['all', 'voice', 'voiceless', 'mc']})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_width = 0.5\n",
    "plt.title('Pre-Test Baseline Recognition Scores per User')\n",
    "plt.bar(finished_post_data['email'], pretest_acc, alpha=1, ecolor='black', capsize=10)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avg # of questions on post-test:\", (1-np.mean(pretest_acc)) * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(16, 4), dpi=300)\n",
    "bar_width = 0.375\n",
    "for i, modality in enumerate(['all', 'voice', 'voiceless', 'mc']):\n",
    "    ax[i].set_title(f'{modality}')\n",
    "    ax[i].set_ylim(0, 1.1)\n",
    "    ax[i].bar(np.arange(len(finished_post_data)) + bar_width,\n",
    "              posttest_recognition_acc[0][modality], width=bar_width, label='post-test recognition')\n",
    "    ax[i].bar(np.arange(len(finished_post_data)) + (2 * bar_width),\n",
    "              posttest_recall_acc[0][modality], width=bar_width, label='post-test recall')\n",
    "\n",
    "    ax[i].set_xticks(np.arange(len(finished_post_data)) + 1.5 * bar_width,\n",
    "                     finished_post_data['email'], rotation=45, ha='right')\n",
    "\n",
    "plt.suptitle(\"Post-Test #1\")\n",
    "plt.legend(bbox_to_anchor=(1.1, 1.05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar_width = 0.25\n",
    "# mean_pretest_acc = {modality: np.mean(accs) for modality, accs in pretest_acc.items()}\n",
    "# mean_posttest_recall_acc = {modality: np.mean(accs) for modality, accs in posttest_recall_acc.items()}\n",
    "# mean_posttest_recognition_acc = {modality: np.mean(accs) for modality, accs in posttest_recognition_acc.items()}\n",
    "\n",
    "# def print_dict(title, d):\n",
    "#     print(f\"{title}:\")\n",
    "#     for k, v in d.items():\n",
    "#         print(f\"  {k}: {v * 100:.2f}%\")\n",
    "# print_dict(\"Recall\", mean_posttest_recall_acc)\n",
    "# print_dict(\"Recognition\", mean_posttest_recognition_acc)\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.bar(np.arange(4), mean_pretest_acc.values(), width=bar_width, label='pre-test recognition')\n",
    "# plt.bar(np.arange(4) + bar_width, mean_posttest_recognition_acc.values(), width=bar_width, label='post-test recognition')\n",
    "# plt.bar(np.arange(4) + 2 * bar_width, mean_posttest_recall_acc.values(), width=bar_width, label='post-test recall')\n",
    "# plt.xticks(np.arange(4) + bar_width, mean_pretest_acc.keys())\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_posttest_recall_acc = [{modality: np.mean(accs) for modality, accs in posttest_recall_acc[i].items()} for i in range(3)] \n",
    "mean_posttest_recall_sem = [{modality: scipy.stats.sem(accs) for modality, accs in posttest_recall_acc[i].items()} for i in range(3)]\n",
    "mean_posttest_recognition_acc = [{modality: np.mean(accs) for modality, accs in posttest_recognition_acc[i].items()} for i in range(3)]\n",
    "mean_posttest_recognition_sem = [{modality: scipy.stats.sem(accs) for modality, accs in posttest_recognition_acc[i].items()} for i in range(3)]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 4), dpi = 300)\n",
    "for i in range(3):\n",
    "  ax[i].bar(np.arange(4), mean_posttest_recall_acc[i].values(), yerr=mean_posttest_recall_sem[i].values(), alpha=1, ecolor='black', capsize=10)\n",
    "  ax[i].axis(ymin=0, ymax=0.8)\n",
    "  ax[i].set_xticks(np.arange(4), mean_posttest_recall_acc[i].keys())\n",
    "  ax[i].set_title(f\"Test {i + 1}\")\n",
    "\n",
    "fig.suptitle('Post-Test Recall Scores per Learning Modality')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 4), dpi = 300)\n",
    "for i in range(3):\n",
    "  ax[i].bar(np.arange(4), mean_posttest_recognition_acc[i].values(), yerr=mean_posttest_recognition_sem[i].values(), alpha=1, ecolor='black', capsize=10)\n",
    "  ax[i].axis(ymin=0, ymax=1)\n",
    "  ax[i].set_xticks(np.arange(4), mean_posttest_recognition_acc[i].keys())\n",
    "  ax[i].set_title(f\"Test {i + 1}\")\n",
    "\n",
    "fig.suptitle('Post-Test Recognition Scores per Learning Modality')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "mean_posttest_recall_acc = [{modality: np.mean(accs) for modality, accs in posttest_recall_acc[i].items()} for i in range(3)] \n",
    "mean_posttest_recall_sem = [{modality: scipy.stats.sem(accs) for modality, accs in posttest_recall_acc[i].items()} for i in range(3)]\n",
    "mean_posttest_recognition_acc = [{modality: np.mean(accs) for modality, accs in posttest_recognition_acc[i].items()} for i in range(3)]\n",
    "mean_posttest_recognition_sem = [{modality: scipy.stats.sem(accs) for modality, accs in posttest_recognition_acc[i].items()} for i in range(3)]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 4), dpi = 300)\n",
    "for i in range(3):\n",
    "  ax[i].violinplot([accs for _, accs in posttest_recall_acc[i].items()])\n",
    "  ax[i].axis(ymin=0, ymax=1)\n",
    "  ax[i].set_xticks(np.arange(4) + 0.5, mean_posttest_recall_acc[i].keys())\n",
    "  ax[i].set_title(f\"Test {i + 1}\")\n",
    "\n",
    "fig.align_xlabels()\n",
    "fig.suptitle('Post-Test Recall Scores per Learning Modality')\n",
    "fig.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 4), dpi = 300)\n",
    "for i in range(3):\n",
    "  ax[i].violinplot([accs for _, accs in posttest_recall_acc[i].items()])\n",
    "  ax[i].axis(ymin=0, ymax=1)\n",
    "  ax[i].set_xticks(np.arange(4) + 0.5, mean_posttest_recognition_acc[i].keys())\n",
    "  ax[i].set_title(f\"Test {i + 1}\")\n",
    "fig.suptitle('Post-Test Recognition Scores per Learning Modality')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "  print(f\"Recall Test {i + 1}\")\n",
    "  for key, accs in posttest_recall_acc[i].items():\n",
    "    ks_result = (scipy.stats.kstest(accs, cdf='norm'))\n",
    "    # print(f\"{key} K-S statistic: {ks_result[0]}\")\n",
    "    print(f\"{key} p-value: {ks_result[1]}\")\n",
    "\n",
    "for i in range(3):\n",
    "  print(f\"Recognition Test {i + 1}\")\n",
    "  for key, accs in posttest_recognition_acc[i].items():\n",
    "    ks_result = (scipy.stats.kstest(accs, cdf='norm'))\n",
    "    # print(f\"{key} K-S statistic: {ks_result[0]}\")\n",
    "    print(f\"{key} p-value: {ks_result[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of all post-test recall responses\n",
    "qs = []\n",
    "for i in range(len(finished_post_data)):\n",
    "    for q in finished_post_data[\"first_posttest\"][i][\"A\"]:\n",
    "        qs.append((q, finished_post_data[\"email\"][i]))\n",
    "\n",
    "    for q in finished_post_data[\"second_posttest\"][i][\"A\"]:\n",
    "        qs.append((q, finished_post_data[\"email\"][i]))\n",
    "\n",
    "    for q in finished_post_data[\"posttestA\"][i].to_dict(\"records\"):\n",
    "        qs.append((q, finished_post_data[\"email\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine duplicate responses\n",
    "def similar(q1, q2):\n",
    "    return q1['QID'] == q2['QID'] and q1['response'] == q2['response']\n",
    "\n",
    "edges = []\n",
    "for i in range(len(qs)):\n",
    "    for j in range(i + 1, len(qs)):\n",
    "        if similar(qs[i][0], qs[j][0]):\n",
    "            edges.append((i, j))\n",
    "\n",
    "g = nx.Graph(edges)\n",
    "clustered_qs = [[qs[i] for i in group] for group in nx.connected_components(g)]\n",
    "print(f\"Original questions: {len(qs)}, reduced questions: {len(clustered_qs)}\")\n",
    "print(f\"{len(qs) / len(clustered_qs):.2f}x reduction in questions to grade!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_qs[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "\n",
    "def add_question(q_group, document):\n",
    "    q = q_group[0][0]\n",
    "    document.add_paragraph(f\"QID: {q['QID']}\")\n",
    "\n",
    "    topic = ' '.join(q[\"QID\"].split(' ')[:-1])\n",
    "    figs = all_qs[\"Figures\"][all_qs[\"Id\"] == q[\"QID\"]].item()\n",
    "    if isinstance(figs, list):\n",
    "        for f in figs:\n",
    "            document.add_picture(f\"../questions/topics/{topic}/{f}\", height=docx.shared.Inches(2))\n",
    "\n",
    "    document.add_paragraph(f\"Question: {all_qs['Question'][all_qs['Id'] == q['QID']].item()}\")\n",
    "    document.add_paragraph(f\"User Response: {q['response']}\")\n",
    "    document.add_paragraph(f\"Correct Response: {all_qs['MultipleChoice'][all_qs['Id'] == q['QID']].item()['Correct']}\")\n",
    "    document.add_paragraph(\"Was User Correct?: \")\n",
    "    document.add_paragraph(\"Comments: \")\n",
    "    document.add_paragraph(f\"SAIL Score: {q['automated_recall_score']}\")\n",
    "    document.add_paragraph(f\"Users: {', '.join([q[1] for q in q_group])}\")\n",
    "    document.add_page_break()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write document for posttest questions\n",
    "np.random.default_rng(42).shuffle(clustered_qs)\n",
    "\n",
    "document = docx.Document()\n",
    "for q_group in tqdm.tqdm(clustered_qs):\n",
    "    add_question(q_group, document)\n",
    "document.save(\"posttest_answers_to_grade.docx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forgetting Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 4), dpi = 300)\n",
    "modalities = ['voice', 'voiceless', 'mc']\n",
    "\n",
    "for modality in modalities:\n",
    "  ax[0].errorbar(['Test 1', 'Test 2', 'Test 3'], \n",
    "                 [mean_posttest_recall_acc[i][modality] for i in range(3)], \n",
    "                #  yerr=[mean_posttest_recall_sem[i][modality] for i in range(3)], \n",
    "                 fmt='o-', label=modality)\n",
    "ax[0].axis(ymin=0, ymax=0.5)\n",
    "ax[0].set_title(\"Post-Test Recall Scores per Modality over Time\")\n",
    "\n",
    "for modality in modalities:\n",
    "  ax[1].errorbar(['Test 1', 'Test 2', 'Test 3'], \n",
    "                 [mean_posttest_recognition_acc[i][modality] for i in range(3)], \n",
    "                #  yerr=[mean_posttest_recognition_sem[i][modality] for i in range(3)], \n",
    "                 fmt='o-')\n",
    "ax[1].set_xticks(np.arange(3), ['Test 1', 'Test 2', 'Test 3'])\n",
    "ax[1].axis(ymin=0, ymax=0.8)\n",
    "ax[1].set_title(\"Post-Test Recognition Scores per Modality over Time\")\n",
    "\n",
    "fig.legend()\n",
    "fig.show()\n",
    "fig.savefig('forgetting_curve_axis=0.png', facecolor='white')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Free Response Grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('../backend/scoring'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import score\n",
    "\n",
    "scorer = score.new_scorer(root='../backend/scoring', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm.trange(len(finished_post_data)):\n",
    "    for posttest_iteration in ['first_posttest', 'second_posttest', \"posttestA\"]:\n",
    "        if posttest_iteration == \"posttestA\":\n",
    "            posttestA = finished_post_data.iloc[i][posttest_iteration]\n",
    "        else:\n",
    "            posttestA = finished_post_data.iloc[i][posttest_iteration]['A']\n",
    "        \n",
    "        # if posttest_iteration == \"posttestA\":\n",
    "        #     print(posttestA.iloc[0]['start'])\n",
    "        for j in range(len(posttestA)):\n",
    "            if posttest_iteration == \"posttestA\":\n",
    "                # print(posttestA.iloc[j])\n",
    "                posttestA.at[j, \"automated_recall_score\"] = scorer.score(posttestA.iloc[j]['QID'], posttestA.iloc[j][\"response\"])\n",
    "                pass\n",
    "            else: \n",
    "                posttestA[j][\"automated_recall_score\"] = scorer.score(posttestA[j]['QID'], posttestA[j][\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(finished_post_data[\"posttestA\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(finished_post_data[\"first_posttest\"][0][\"A\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for posttest_iteration in ['posttestA', 'first_posttest', 'second_posttest']:\n",
    "    print(f\"Writing scores for {posttest_iteration}\")\n",
    "    for email, posttest in tqdm.tqdm(list(zip(finished_post_data['email'], finished_post_data[posttest_iteration]))):\n",
    "        with open(f'../backend/users/{email}.yaml') as f:\n",
    "            user_data = yaml.safe_load(f)\n",
    "        \n",
    "        if posttest_iteration != \"posttestA\": \n",
    "            for q, posttest_q in zip(user_data[posttest_iteration][\"A\"], posttest[\"A\"]):\n",
    "                q['automated_recall_score'] = posttest_q[\"automated_recall_score\"]\n",
    "        else:\n",
    "            posttest_array = [posttest.iloc[i].to_dict() for i in range(posttest.shape[0])]\n",
    "            for q, posttest_q in zip(user_data[posttest_iteration], posttest_array):\n",
    "                q['automated_recall_score'] = int(posttest_q[\"automated_recall_score\"])\n",
    "        \n",
    "        with open(f'../backend/users/{email}.yaml', 'w') as f:\n",
    "            yaml.dump(user_data, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Difficulties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_difficulties = pd.read_csv('question_difficulties.csv')\n",
    "plt.title('Percentage who answered question correctly')\n",
    "plt.hist(question_difficulties['Percent who answered correctly'], bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sleep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_post_data['sleepData'].to_numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a8a45cbae980bda797f4c4d96a25cc23e529283e4e0fc1b6546e39186b18bc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
