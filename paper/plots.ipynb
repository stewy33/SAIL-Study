{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import yaml\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_qs = []\n",
    "for topic in glob.glob('../questions/topics/*'):\n",
    "    if 'Demo' not in topic:\n",
    "        with open(f\"{topic}/questions.yaml\") as f:\n",
    "            all_qs.extend(yaml.safe_load(f))\n",
    "\n",
    "all_qs = pd.DataFrame(all_qs)\n",
    "correct_answers = dict(zip(all_qs['Id'], all_qs['MultipleChoice'].apply(lambda r: r['Correct'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_data = []\n",
    "\n",
    "for file in glob.glob('../backend/users/*.yaml'):\n",
    "    if 'demo' not in file:\n",
    "        with open(file) as f:\n",
    "            data = yaml.safe_load(f)\n",
    "\n",
    "        if 'pretest' in data:\n",
    "            data['pretest'] = pd.DataFrame(data['pretest'])\n",
    "        if 'questionSchedule' not in data:\n",
    "            continue\n",
    "        \n",
    "        qSchedule = []\n",
    "        for i, day in enumerate(data['questionSchedule']):\n",
    "            for j, q in enumerate(day):\n",
    "                q['day'] = i\n",
    "                q['numInDay'] = j\n",
    "                qSchedule.append(q)\n",
    "        data['questionSchedule'] = pd.DataFrame(qSchedule)\n",
    "\n",
    "        if 'posttestA' in data:\n",
    "            data['posttestA'] = pd.DataFrame(data['posttestA'])\n",
    "        if 'posttestB' in data:\n",
    "            data['posttestB'] = pd.DataFrame(data['posttestB'])\n",
    "        if 'sleepData' in data:\n",
    "            data['sleepData'] = np.array([float(d['numHours']) for d in data['sleepData']])\n",
    "\n",
    "        all_user_data.append(data)\n",
    "\n",
    "all_user_data = pd.DataFrame(all_user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all emails (after the @ sign) from all_user_data[\"email\"]\n",
    "all_user_data[\"email\"].apply(lambda x: x.split(\"@\")[1]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_emails = []\n",
    "bad_emails = ['madison.evans@som.umaryland.edu', 'puja.patel@som.umaryland.edu', 'kran2@jh.edu', 'charles1@usf.edu']\n",
    "\n",
    "'''\n",
    "# shamil34@jhmi.edu 0.3103448275862069\n",
    "# shannon.hanggodo@quinnipiac.edu 0.26153846153846155\n",
    "madison.evans@som.umaryland.edu 0.0\n",
    "# aweitzn1@jh.edu 0.3142857142857143\n",
    "# mkubica1@jhmi.edu 0.30158730158730157\n",
    "# mmarani1@jhmi.edu 0.2564102564102564\n",
    "# aatkin31@jhmi.edu 0.35526315789473684\n",
    "puja.patel@som.umaryland.edu 0.04225352112676056\n",
    "# slin86@jhmi.edu 0.328125\n",
    "# kkuhn14@jh.edu 0.3442622950819672\n",
    "# kzhu24@jhmi.edu 0.18181818181818182\n",
    "# xdai9@jhmi.edu 0.3770491803278688\n",
    "# lyoung1090@gmail.com 0.22857142857142856\n",
    "charles1@usf.edu 0.20588235294117646\n",
    "kran2@jh.edu 0.0\n",
    "'''\n",
    "\n",
    "finished_study_data = all_user_data[all_user_data['status'].isin(['posttestDone', 'studyDone', 'posttestPartADone']) & ~all_user_data['email'].isin(bad_emails)]\n",
    "finished_users = finished_study_data['email'].unique()\n",
    "print(f\"Participants who finished study portion: {len(finished_study_data)}\\n{finished_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_post_data = all_user_data[all_user_data['status'].isin(['posttestDone']) & ~all_user_data['email'].isin(bad_emails)].reset_index()\n",
    "print(f\"Participants who finished all post-tests: {len(finished_post_data)}\\n{finished_post_data['email'].to_list()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraction contested\n",
    "\n",
    "Calculating fraction contested overall and per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../backend/scoring/contestedEvaluations.yaml') as f:\n",
    "    contested_evaluations = pd.DataFrame(yaml.safe_load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "users_seen = set()\n",
    "for _, row in contested_evaluations.iterrows():\n",
    "    if row['user'] not in finished_users:\n",
    "      continue\n",
    "    user_row = finished_post_data[finished_post_data['email'] == row['user']]\n",
    "    qSched = user_row['questionSchedule'].iloc[0]\n",
    "    qSched_row = qSched[qSched['qid'] == row['QID']].iloc[0]\n",
    "    if qSched_row['modality'] == 'mc':\n",
    "      print('oops..')\n",
    "      print(row)\n",
    "      contested_evaluations = contested_evaluations[~((contested_evaluations['user'] == row['user']) & (contested_evaluations['timestamp'] == row['timestamp']))]\n",
    "\n",
    "    if row['user'] not in users_seen:\n",
    "      count = count + 1\n",
    "      users_seen.add(row['user'])\n",
    "\n",
    "print(f\"{count} users contested at least 1 score\")\n",
    "contested_evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_questions = 120\n",
    "\n",
    "frac_contested = len(contested_evaluations[contested_evaluations['user'].isin(finished_users)]) / (len(finished_users) * total_questions)\n",
    "frac_contested_per_user = {user: (contested_evaluations['user'] == user).sum() / total_questions for user in finished_users}\n",
    "print(f\"Percentage of overall responses contested: {100 * frac_contested:.2f}% +/- {100 * scipy.stats.sem([x for x in frac_contested_per_user.values()]):.2f}%\")\n",
    "\n",
    "plt.title('Percentage of responses contested per user')\n",
    "plt.bar(frac_contested_per_user.keys(), np.array(list(frac_contested_per_user.values())) * 100)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contested_evaluations['correct'] = [correct_answers[qid] for qid in contested_evaluations['QID']]\n",
    "contested_evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = contested_evaluations['QID'].map(lambda qid: all_qs['Question'][all_qs['Id'] == qid].item())\n",
    "contested_evaluations_save = pd.DataFrame({\n",
    "    'QID': contested_evaluations['QID'],\n",
    "    'Figures': ['']*len(contested_evaluations),\n",
    "    'Question': questions,\n",
    "    'User Response': contested_evaluations['userResponse'],\n",
    "    'Correct Response': contested_evaluations['correct'],\n",
    "    'Was User Correct?': ['']*len(contested_evaluations),\n",
    "    'Comments': ['']*len(contested_evaluations),\n",
    "    'SAIL Score': contested_evaluations['score'],\n",
    "    'User': contested_evaluations['user']\n",
    "})\n",
    "\n",
    "writer = pd.ExcelWriter('contested_evaluations.xlsx', engine='xlsxwriter')\n",
    "contested_evaluations_save.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.sheets['Sheet1'].set_default_row(300)\n",
    "\n",
    "for i, qid in enumerate(contested_evaluations['QID']):\n",
    "    topic = ' '.join(qid.split(' ')[:-1])\n",
    "    figs = all_qs['Figures'][all_qs['Id'] == qid].item()\n",
    "    if isinstance(figs, list):\n",
    "        for f in figs:\n",
    "            writer.sheets['Sheet1'].insert_image(i+1, 2, f'../questions/topics/{topic}/{f}')\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "\n",
    "document = docx.Document()\n",
    "questions = contested_evaluations['QID'].map(lambda qid: all_qs['Question'][all_qs['Id'] == qid].item())\n",
    "\n",
    "for i in range(len(contested_evaluations)):\n",
    "    r = document.add_paragraph().add_run()\n",
    "    r.add_text(f\"QID: {contested_evaluations['QID'][i]}\")\n",
    "    r.add_text(f\"Question: {questions[i]}\")\n",
    "    r.add_text(f\"User Response {contested_evaluations['userResponse'][i]}\")\n",
    "    r.add_text(f\"Correct Response: {contested_evaluations['correct'][i]}\")\n",
    "\n",
    "document.save('contested_evaluations.docx')\n",
    "\n",
    "contested_evaluations_save = pd.DataFrame({\n",
    "    'QID': contested_evaluations['QID'],\n",
    "    'Figures': ['']*len(contested_evaluations),\n",
    "    'Question': questions,\n",
    "    'User Response': contested_evaluations['userResponse'],\n",
    "    'Correct Response': contested_evaluations['correct'],\n",
    "    'Was User Correct?': ['']*len(contested_evaluations),\n",
    "    'Comments': ['']*len(contested_evaluations),\n",
    "    'SAIL Score': contested_evaluations['score'],\n",
    "    'User': contested_evaluations['user']\n",
    "})\n",
    "\n",
    "writer = pd.ExcelWriter('contested_evaluations.xlsx', engine='xlsxwriter')\n",
    "contested_evaluations_save.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.sheets['Sheet1'].set_default_row(300)\n",
    "\n",
    "for i, qid in enumerate(contested_evaluations['QID']):\n",
    "    topic = ' '.join(qid.split(' ')[:-1])\n",
    "    figs = all_qs['Figures'][all_qs['Id'] == qid].item()\n",
    "    if isinstance(figs, list):\n",
    "        for f in figs:\n",
    "            writer.sheets['Sheet1'].insert_image(i+1, 2, f'../questions/topics/{topic}/{f}')\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability of voice transcription\n",
    "\n",
    "Calculating fraction of times they edited voice transcription, overall and per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_voice_responses = 60\n",
    "voice_edited_responses = finished_study_data['questionSchedule'].apply(\n",
    "    lambda qs: qs[(qs['modality'] == 'voice') & (qs['userResponse'].str.strip().str.lower() != qs['originalResponse'].str.strip().str.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_edited_per_user = voice_edited_responses.apply(len) / num_voice_responses\n",
    "\n",
    "print(f\"Percentage of overall responses edited: {100 * frac_edited_per_user.mean():.2f}% +/- {100 * frac_edited_per_user.sem():.2f}%\")\n",
    "\n",
    "plt.title('Percentage of responses edited per user')\n",
    "plt.bar(finished_study_data['email'], frac_edited_per_user * 100)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study example response edits\n",
    "voice_edited_responses[finished_study_data['email'] == 'andrewbharris@jhmi.edu'].squeeze()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study Performance per Modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_study_accuracy(qs, modality=None):\n",
    "    if modality is not None:\n",
    "        qs = qs[qs['modality'] == modality]\n",
    "    return qs['score'].mean()\n",
    "\n",
    "overall = finished_study_data['questionSchedule'].apply(lambda qs: calc_study_accuracy(qs)).mean()\n",
    "voice = finished_study_data['questionSchedule'].apply(lambda qs: calc_study_accuracy(qs, 'voice')).mean()\n",
    "voiceless = finished_study_data['questionSchedule'].apply(lambda qs: calc_study_accuracy(qs, 'voiceless')).mean()\n",
    "mc = finished_study_data['questionSchedule'].apply(lambda qs: calc_study_accuracy(qs, 'mc')).mean()\n",
    "\n",
    "sems = [finished_study_data['questionSchedule'].apply(lambda qs: calc_study_accuracy(qs, modality)).sem() for modality in [None, \"voice\", \"voiceless\", \"mc\"]]\n",
    "\n",
    "plt.title('Accuracy during study per modality')\n",
    "plt.bar(['Overall', 'Voice', 'Voiceless', 'MC'], [overall, voice, voiceless, mc], yerr=sems, alpha=1, ecolor='black', capsize=10)\n",
    "plt.plot()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement per Modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_test_accuracies(test_type, modality, test_method):\n",
    "    accs = []\n",
    "    for _, row in finished_post_data.iterrows():\n",
    "        qSched = row['questionSchedule']\n",
    "        if modality == 'all':\n",
    "            modality_qids = qSched['qid'].unique()\n",
    "        else:\n",
    "            modality_qids = qSched['qid'][qSched['modality'] == modality].unique()\n",
    "\n",
    "        test = row[test_type.split(\".\")[0]]\n",
    "        if \".\" in test_type:\n",
    "            test = test[test_type.split(\".\")[1]]\n",
    "        \n",
    "        test = pd.DataFrame(test)\n",
    "        if test_method == 'recognition':\n",
    "            accuracy = np.mean([test['response'][i] == correct_answers[test['QID'][i]]\n",
    "                                for i in range(len(test)) if test['QID'][i] in modality_qids])\n",
    "        elif test_method == 'recall':\n",
    "            accuracy = np.mean([test['automated_recall_score'][i]\n",
    "                                for i in range(len(test)) if test['QID'][i] in modality_qids])\n",
    "        \n",
    "        accs.append(accuracy)\n",
    "    \n",
    "    return accs\n",
    "\n",
    "pretest_acc = calc_test_accuracies('pretest', 'all', test_method=\"recognition\")\n",
    "posttest_recall_acc = []\n",
    "posttest_recognition_acc = []\n",
    "\n",
    "for posttest_iteration in ['first_posttest', 'second_posttest', 'posttest']:\n",
    "    recall_testname = f'{posttest_iteration}.A'\n",
    "    recog_testname = f'{posttest_iteration}.B'\n",
    "    if posttest_iteration == \"posttest\":\n",
    "        recall_testname = \"posttestA\"\n",
    "        recog_testname = \"posttestB\"\n",
    "    \n",
    "    posttest_recall_acc.append({modality: calc_test_accuracies(recall_testname, modality, test_method='recall')\n",
    "                                for modality in ['all', 'voice', 'voiceless', 'mc']})\n",
    "    posttest_recognition_acc.append({modality: calc_test_accuracies(recog_testname, modality, test_method=\"recognition\")\n",
    "                                     for modality in ['all', 'voice', 'voiceless', 'mc']})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_test_stats(test_type, modality):\n",
    "    stats = []\n",
    "    for _, row in finished_post_data.iterrows():\n",
    "        qSched = row['questionSchedule']\n",
    "        if modality == 'all':\n",
    "            modality_qids = qSched['qid'].unique()\n",
    "        else:\n",
    "            modality_qids = qSched['qid'][qSched['modality'] == modality].unique()\n",
    "\n",
    "        test = row[test_type.split(\".\")[0]]\n",
    "        if \".\" in test_type:\n",
    "            test = test[test_type.split(\".\")[1]]\n",
    "        \n",
    "        test = pd.DataFrame(test)\n",
    "        dataaaa = []\n",
    "        dataaaa = [i for i in range(len(test)) if test['QID'][i] in modality_qids]\n",
    "        \n",
    "        stats.append(len(dataaaa))\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "for modality in [\"all\", \"voice\", \"voiceless\", \"mc\"]:\n",
    "    # calculate how many questions were included for each post-test test\n",
    "    arr = calc_test_stats(\"first_posttest.A\", modality)\n",
    "    # print median, 25th percentile, 75th percentile\n",
    "    print(modality, np.median(arr), np.percentile(arr, 25), np.percentile(arr, 75))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "bar_width = 0.5\n",
    "plt.title('Pre-Test Baseline Recognition Scores per User')\n",
    "plt.bar(finished_post_data['email'], pretest_acc, alpha=1, ecolor='black', capsize=10)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(16, 4), dpi=300)\n",
    "bar_width = 0.375\n",
    "for i, modality in enumerate(['all', 'voice', 'voiceless', 'mc']):\n",
    "    ax[i].set_title(f'{modality}' if modality != 'all' else \"overall\")\n",
    "    ax[i].set_ylim(0, 1.1)\n",
    "    ax[i].bar(np.arange(len(finished_post_data)) + bar_width,\n",
    "              posttest_recognition_acc[0][modality], width=bar_width, label='post-test recognition')\n",
    "    ax[i].bar(np.arange(len(finished_post_data)) + (2 * bar_width),\n",
    "              posttest_recall_acc[0][modality], width=bar_width, label='post-test recall')\n",
    "\n",
    "    ax[i].set_xticks(np.arange(len(finished_post_data)) + 1.5 * bar_width,\n",
    "                     finished_post_data['email'], rotation=45, ha='right')\n",
    "\n",
    "plt.suptitle(\"Post-Test #1\")\n",
    "plt.legend(bbox_to_anchor=(1.1, 1.05))\n",
    "plt.show()\n",
    "\n",
    "for i, score in enumerate(posttest_recall_acc[0][\"voice\"]):\n",
    "    if score < 0.4:\n",
    "        print(finished_post_data['email'][i], score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_posttest_recall_acc = [{modality: np.mean(np.subtract(accs, 0)) for modality, accs in posttest_recall_acc[i].items() if modality != \"all\"} for i in range(3)] \n",
    "mean_posttest_recall_sem = [{modality: scipy.stats.sem(np.subtract(accs, 0)) for modality, accs in posttest_recall_acc[i].items() if modality != \"all\"} for i in range(3)]\n",
    "mean_posttest_recognition_acc = [{modality: np.mean(np.subtract(accs, 0)) for modality, accs in posttest_recognition_acc[i].items() if modality != \"all\"} for i in range(3)]\n",
    "mean_posttest_recognition_sem = [{modality: scipy.stats.sem(np.subtract(accs, 0)) for modality, accs in posttest_recognition_acc[i].items() if modality != \"all\"} for i in range(3)]\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"Test {i + 1}\")\n",
    "    for modality in [\"voice\", \"voiceless\", \"mc\"]:\n",
    "      print(f\"{modality}\")\n",
    "      print(f\"Recall {mean_posttest_recall_acc[i][modality]} +- {mean_posttest_recall_sem[i][modality]}\")\n",
    "      print(f\"Recog {mean_posttest_recognition_acc[i][modality]} +- {mean_posttest_recognition_sem[i][modality]}\")\n",
    "    \n",
    "# mean_posttest_recall_acc = [{modality: np.mean(accs) for modality, accs in posttest_recall_acc[i].items() if modality != \"all\"} for i in range(3)] \n",
    "# mean_posttest_recall_sem = [{modality: scipy.stats.sem(accs) for modality, accs in posttest_recall_acc[i].items() if modality != \"all\"} for i in range(3)]\n",
    "# mean_posttest_recognition_acc = [{modality: np.mean(accs) for modality, accs in posttest_recognition_acc[i].items() if modality != \"all\"} for i in range(3)]\n",
    "# mean_posttest_recognition_sem = [{modality: scipy.stats.sem(accs) for modality, accs in posttest_recognition_acc[i].items() if modality != \"all\"} for i in range(3)]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 4), dpi = 300)\n",
    "for i in range(3):\n",
    "  ax[i].bar(np.arange(3), mean_posttest_recall_acc[i].values(), yerr=mean_posttest_recall_sem[i].values(), alpha=1, ecolor='black', capsize=10)\n",
    "  neg_vals = []\n",
    "  for val in mean_posttest_recall_acc[i].values():\n",
    "    if val > 0:\n",
    "      neg_vals.append(0)\n",
    "    else:\n",
    "      neg_vals.append(val)\n",
    "  ax[i].bar(np.arange(3), neg_vals, alpha=1, color='r')\n",
    "  ax[i].axis(ymin=0, ymax=1)\n",
    "  ax[i].set_xticks(np.arange(3), mean_posttest_recall_acc[i].keys())\n",
    "  ax[i].set_title(f\"Test {i + 1}\")\n",
    "\n",
    "fig.suptitle('Post-Test Recall Scores per Learning Modality')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 4), dpi = 300)\n",
    "for i in range(3):\n",
    "  ax[i].bar(np.arange(3), mean_posttest_recognition_acc[i].values(), yerr=mean_posttest_recognition_sem[i].values(), alpha=1, ecolor='black', capsize=10)\n",
    "  neg_vals = []\n",
    "  for val in mean_posttest_recognition_acc[i].values():\n",
    "    if val > 0:\n",
    "      neg_vals.append(0)\n",
    "    else:\n",
    "      neg_vals.append(val)\n",
    "  ax[i].bar(np.arange(3), neg_vals, alpha=1, color='r')\n",
    "  ax[i].axis(ymin=0, ymax=1)\n",
    "  ax[i].set_xticks(np.arange(3), mean_posttest_recognition_acc[i].keys())\n",
    "  ax[i].set_title(f\"Test {i + 1}\")\n",
    "\n",
    "fig.suptitle('Post-Test Recognition Scores per Learning Modality')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(16, 4), dpi = 300)\n",
    "for i in range(3):\n",
    "  ax[i].violinplot([accs for _, accs in posttest_recall_acc[i].items()])\n",
    "  ax[i].axis(ymin=-0.1, ymax=1.1)\n",
    "  ax[i].set_xticks(np.arange(4) + 0.5, posttest_recall_acc[i].keys())\n",
    "  ax[i].set_title(f\"Test {i + 1}\")\n",
    "\n",
    "fig.align_xlabels()\n",
    "fig.suptitle('Post-Test Recall Scores per Learning Modality')\n",
    "fig.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 4), dpi = 300)\n",
    "for i in range(3):\n",
    "  ax[i].violinplot([accs for _, accs in posttest_recognition_acc[i].items()])\n",
    "  ax[i].axis(ymin=-0.1, ymax=1.1)\n",
    "  ax[i].set_xticks(np.arange(4) + 0.5, posttest_recognition_acc[i].keys())\n",
    "  ax[i].set_title(f\"Test {i + 1}\")\n",
    "fig.suptitle('Post-Test Recognition Scores per Learning Modality')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "  print(f\"Recall Test {i + 1}\")\n",
    "  for key, accs in posttest_recall_acc[i].items():\n",
    "    ks_result = scipy.stats.shapiro(accs)\n",
    "    print(f\"{key} p-value: {ks_result[1]}\")\n",
    "\n",
    "for i in range(3):\n",
    "  print(f\"Recognition Test {i + 1}\")\n",
    "  for key, accs in posttest_recognition_acc[i].items():\n",
    "    ks_result = scipy.stats.shapiro(accs)\n",
    "    print(f\"{key} p-value: {ks_result[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forgetting Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modalities_dict = {'voice': 'Oral', 'voiceless': 'Typed', 'mc': 'MC'}\n",
    "modalities = ['voice', 'voiceless', 'mc']\n",
    "\n",
    "mean_posttest_recall_acc = [{modality: np.mean(accs) for modality, accs in posttest_recall_acc[i].items()} for i in range(3)] \n",
    "mean_posttest_recall_sem = [{modality: scipy.stats.sem(accs) for modality, accs in posttest_recall_acc[i].items()} for i in range(3)]\n",
    "mean_posttest_recognition_acc = [{modality: np.mean(accs) for modality, accs in posttest_recognition_acc[i].items()} for i in range(3)]\n",
    "mean_posttest_recognition_sem = [{modality: scipy.stats.sem(accs) for modality, accs in posttest_recognition_acc[i].items()} for i in range(3)]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 4), dpi=600)\n",
    "colors = ['#1b9e77', '#0295D9', '#7570b3']\n",
    "\n",
    "for i, modality in enumerate(modalities):\n",
    "  ax[0].errorbar(['Day 1', 'Day 7', 'Day 60'], \n",
    "                 [mean_posttest_recall_acc[i][modality] for i in range(3)],\n",
    "                 yerr=[mean_posttest_recall_sem[i][modality] for i in range(3)], \n",
    "                 fmt='o-', label=modalities_dict[modality], color=colors[i],\n",
    "                 linewidth=2)\n",
    "ax[0].axis(ymin=0, ymax=0.55)\n",
    "ax[0].set_title(\"Recall\")\n",
    "\n",
    "# ax[0].set_ylabel('Rate', labelpad=10)\n",
    "\n",
    "\n",
    "for i, modality in enumerate(modalities):\n",
    "  ax[1].errorbar(['Day 1', 'Day 7', 'Day 60'], \n",
    "                 [mean_posttest_recognition_acc[i][modality] for i in range(3)], \n",
    "                 yerr=[mean_posttest_recognition_sem[i][modality] for i in range(3)], \n",
    "                 fmt='o-', color=colors[i],\n",
    "                 linewidth=2)\n",
    "# ax[1].set_xticks(np.arange(3), ['Day 1', 'Day 7', 'Day 60'])\n",
    "ax[1].axis(ymin=0, ymax=0.8)\n",
    "ax[1].set_title(\"Recognition\")\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=False, ncol=5)\n",
    "\n",
    "# less padding for the legend\n",
    "plt.subplots_adjust(bottom=0)\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "fig.show()\n",
    "fig.savefig('forgetting_curve.png', dpi=600, bbox_inches='tight', transparent=False, pad_inches=0.1, facecolor='white')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.graphpad.com/guides/prism/latest/statistics/\n",
    "# https://www.marsja.se/repeated-measures-anova-in-python-using-statsmodels/\n",
    "\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "from scipy.stats import wilcoxon, friedmanchisquare, ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two way RM ANOVA to test whether modality, time of post-test, or interaction between modality and time of post-test has an effect on test accuracy \n",
    "# interaction null hypothesis is that the difference between the modalities is the same for all three post-tests\n",
    "\n",
    "recall_twoway = [['subject_id', 'score', 'modality', 'test_iter']]\n",
    "for test_iter in range(3):\n",
    "    for modality, accs in posttest_recall_acc[test_iter].items():\n",
    "      if modality == \"all\" or modality == \"voiceless\":\n",
    "         continue\n",
    "      for id, score in enumerate(accs):\n",
    "         recall_twoway.append([id, score, modality, test_iter])\n",
    "\n",
    "recog_twoway = [['subject_id', 'score', 'modality', 'test_iter']]\n",
    "for test_iter in range(3):\n",
    "    for modality, accs in posttest_recognition_acc[test_iter].items():\n",
    "      if modality == \"all\" or modality == \"voiceless\":\n",
    "         continue\n",
    "      for id, score in enumerate(accs):\n",
    "         recog_twoway.append([id, score, modality, test_iter])\n",
    "\n",
    "recog_twoway_df = pd.DataFrame(recog_twoway[1:], columns=recog_twoway[0])\n",
    "recall_twoway_df = pd.DataFrame(recall_twoway[1:], columns=recall_twoway[0])\n",
    "\n",
    "anova2way = AnovaRM(recog_twoway_df, 'score', 'subject_id', within=['modality', 'test_iter'])\n",
    "res_recog = anova2way.fit()\n",
    "anova2way = AnovaRM(recall_twoway_df, 'score', 'subject_id', within=['modality', 'test_iter'])\n",
    "res_recall = anova2way.fit()\n",
    "print(\"Recall\", res_recall)\n",
    "print(\"Recognition\", res_recog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 3):\n",
    "  print(f\"Test {i + 1}\")\n",
    "  anova = AnovaRM(recog_twoway_df[recog_twoway_df['test_iter'] == i], 'score', 'subject_id', within=['modality'])\n",
    "  res_recog = anova.fit()\n",
    "\n",
    "  anova = AnovaRM(recall_twoway_df[recall_twoway_df['test_iter'] == i], 'score', 'subject_id', within=['modality'])\n",
    "  res_recall = anova.fit()\n",
    "\n",
    "  print(\"Recall\", res_recall)\n",
    "  print(\"Recognition\", res_recog)\n",
    "  print('-----------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Testing the durability/robustness of learning\"\n",
    "# Friedman to test whether modality has an effect on accuracy in the final, 2-month test: Test 3\n",
    "for i in range(0, 3):\n",
    "  print(f\"Test {i + 1}\")\n",
    "  recall_table = []\n",
    "  for modality, accs in posttest_recall_acc[i].items():\n",
    "    if modality == \"all\":\n",
    "      continue\n",
    "    recall_table.append(accs)\n",
    "\n",
    "  recog_table = []\n",
    "  for modality, accs in posttest_recognition_acc[i].items():\n",
    "    if modality == \"all\":\n",
    "      continue\n",
    "    recog_table.append(accs)\n",
    "\n",
    "  print(\"Recall\", friedmanchisquare(recall_table[0], recall_table[1], recall_table[2]))\n",
    "  print(\"Recognition\", friedmanchisquare(recog_table[0], recog_table[1], recog_table[2]))\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-hoc Analysis for Recall\n",
    "\n",
    "# test_func = ttest_rel\n",
    "test_func = wilcoxon\n",
    "\n",
    "# # How to design post-hoc test to explore which two timepoints have a different difference in accuracy between modalities?\n",
    "# # Wilcoxon to compare (Voice-MC) at time 1 vs (Voice-MC) at time 3 ? Note, these values have a + or - sign.\n",
    "# print(\"Wilcoxon to compare (Voice-MC) at time 1 vs (Voice-MC) at time 3\")\n",
    "# stat, p = test_func(np.subtract(posttest_recall_acc[2]['voice'], posttest_recall_acc[2]['mc']), \n",
    "#                     np.subtract(posttest_recall_acc[0]['voice'], posttest_recall_acc[0]['mc']))\n",
    "# print(f\"(Voice-MC) at time 3 != at time 1: {p}\")\n",
    "\n",
    "# print(\"Wilcoxon to compare (Voice-MC) at time 2 vs (Voice-MC) at time 1\")\n",
    "# stat, p = test_func(np.subtract(posttest_recall_acc[1]['voice'], posttest_recall_acc[1]['mc']), \n",
    "#                     np.subtract(posttest_recall_acc[0]['voice'], posttest_recall_acc[0]['mc']))\n",
    "# print(f\"(Voice-MC) at time 2 != at time 1: {p}\")\n",
    "\n",
    "\n",
    "print(\"--------------------\")\n",
    "\n",
    "print(\"Recall\")\n",
    "stat, p = test_func(posttest_recall_acc[0]['voice'], posttest_recall_acc[0]['mc'])\n",
    "print(f\"Voice != MC (test 1): {p}\")\n",
    "\n",
    "stat, p = test_func(posttest_recall_acc[0]['voice'], posttest_recall_acc[0]['voiceless'])\n",
    "print(f\"Voice != Voiceless (test 1): {p}\")\n",
    "\n",
    "stat, p = test_func(posttest_recall_acc[1]['voice'], posttest_recall_acc[1]['mc'])\n",
    "print(f\"Voice != MC (test 2): {p}\")\n",
    "\n",
    "stat, p = test_func(posttest_recall_acc[1]['voice'], posttest_recall_acc[1]['voiceless'])\n",
    "print(f\"Voice != Voiceless (test 2): {p}\")\n",
    "\n",
    "stat, p = test_func(posttest_recall_acc[2]['voice'], posttest_recall_acc[2]['mc'])\n",
    "print(f\"Voice != MC (test 3): {p}\")\n",
    "\n",
    "stat, p = test_func(posttest_recall_acc[2]['voice'], posttest_recall_acc[2]['voiceless'])\n",
    "print(f\"Voice != Voiceless (test 3): {p}\")\n",
    "\n",
    "print(\"--------------------\")\n",
    "print(\"Recognition\")\n",
    "\n",
    "# Post-hoc Analysis for Recognition\n",
    "stat, p = test_func(posttest_recognition_acc[0]['mc'], posttest_recognition_acc[0]['voice'])\n",
    "print(f\"MC != Voice (test 1): {p}\")\n",
    "\n",
    "stat, p = test_func(posttest_recognition_acc[1]['mc'], posttest_recognition_acc[1]['voice'])\n",
    "print(f\"MC != Voice (test 2): {p}\")\n",
    "\n",
    "stat, p = test_func(posttest_recognition_acc[2]['mc'], posttest_recognition_acc[2]['voice'])\n",
    "print(f\"MC != Voice (test 3): {p}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participant characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Participant characteristics\n",
    "participantInfo = pd.read_csv('/home/arpan/SAIL/backend/users/participants.csv')\n",
    "\n",
    "# level of schooling\n",
    "# birthday\n",
    "# gender\n",
    "# first language\n",
    "# preferred language\n",
    "# country of origin\n",
    "\n",
    "# filter participantInfo to only include participants who have completed the study\n",
    "participantInfo = participantInfo[participantInfo['Username'].isin(finished_post_data['email'])]\n",
    "participantInfo.reset_index(inplace=True, drop=True)\n",
    "\n",
    "import datetime\n",
    "def calculate_age(x):\n",
    "  bday = x['Birthday']\n",
    "  try:\n",
    "    age = (datetime.datetime.now() - datetime.datetime.strptime(bday, \"%Y-%m-%d\")).days / 365\n",
    "  except Exception as e:\n",
    "    print(x['Username'], bday)\n",
    "    age = 26\n",
    "  return age\n",
    "\n",
    "participantInfo['age'] = participantInfo.apply(lambda x: calculate_age(x), axis=1)\n",
    "\n",
    "\n",
    "# in participantInfo['Level of Schooling'], replaces instances of \"M4 at UMSOM\" with \"Medical Student - Year 4\" and \"MS2\" with \"Medical Student - Year 2\"\n",
    "participantInfo['Level of Schooling'] = participantInfo['Level of Schooling'].replace(\"M4 at UMSOM\", \"Medical Student - Year 4\")\n",
    "participantInfo['Level of Schooling'] = participantInfo['Level of Schooling'].replace(\"MS2\", \"Medical Student - Year 2\")\n",
    "\n",
    "\n",
    "participantInfo\n",
    "\n",
    "# create a XLSX file that can have multiple sheets\n",
    "writer = pd.ExcelWriter('participantInfo.xlsx', engine='xlsxwriter')\n",
    "\n",
    "for col in [\"Level of Schooling\", \"age\", \"Gender\", \"In which country were you born?\", \"What was the first language you learned?\", \"What is your preferred language?\"]:\n",
    "  if col == \"age\":\n",
    "    # create a table of summary stats for participantInfo['age'] and save it to a sheet of the xlsx files\n",
    "    summary_stats = participantInfo['age'].describe()\n",
    "    # add SEM to summary stats\n",
    "    summary_stats['SEM'] = participantInfo['age'].sem()\n",
    "    summary_stats.to_excel(writer, sheet_name=col)\n",
    "    continue\n",
    "\n",
    "  value_counts = participantInfo[col].value_counts(normalize=True)\n",
    "  value_counts = value_counts.to_frame()\n",
    "  value_counts['count'] = participantInfo[col].value_counts()\n",
    "  print(value_counts)\n",
    "  \n",
    "  # save value counts to a sheet of the xlsx file\n",
    "  col = col.replace(\" \", \"_\")\n",
    "  col = col.replace(\"?\", \"\")\n",
    "  col = col[:31]\n",
    "  value_counts.to_excel(writer, sheet_name=col)\n",
    "\n",
    "# save the xlsx file\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "# create pandas df from a csv file\n",
    "participantInfo = pd.read_csv('/home/arpan/SAIL/backend/users/participants.csv')\n",
    "\n",
    "# trainingDict = {\n",
    "#   \"Orthopedic Resident - Year 5\": 9,\n",
    "#   \"Orthopedic Resident - Year 4\": 8,\n",
    "#   \"Orthopedic Resident - Year 3\": 7,\n",
    "#   \"Orthopedic Resident - Year 2\": 6,\n",
    "#   \"Orthopedic Resident - Year 1\": 5,\n",
    "#   \"Medical Student - Year 4\": 4,\n",
    "#   \"Medical Student - Year 3\": 3,\n",
    "#   \"Medical Student - Year 2\": 2,\n",
    "#   \"Medical Student - Year 1\": 1,\n",
    "#   \"MS2\": 2,\n",
    "#   \"M4 at UMSOM\": 4,\n",
    "#   \"M2\": 2,\n",
    "#   \"Medical Student - MS3\": 3\n",
    "# }\n",
    "\n",
    "trainingDict = {\n",
    "  \"Orthopedic Resident - Year 5\": 6,\n",
    "  \"Orthopedic Resident - Year 4\": 5,\n",
    "  \"Orthopedic Resident - Year 3\": 4,\n",
    "  \"Orthopedic Resident - Year 2\": 3,\n",
    "  \"Orthopedic Resident - Year 1\": 2,\n",
    "  \"Medical Student - Year 4\": 1,\n",
    "  \"Medical Student - Year 3\": 0.5,\n",
    "  \"Medical Student - Year 2\": 0,\n",
    "  \"Medical Student - Year 1\": 0,\n",
    "  \"MS2\": 0,\n",
    "  \"M4 at UMSOM\": 1,\n",
    "  \"M2\": 0,\n",
    "  \"Medical Student - MS3\": 0.5\n",
    "}\n",
    "\n",
    "mat = []\n",
    "for _, row in finished_post_data.iterrows():\n",
    "  qSched = row['questionSchedule']\n",
    "  sleepData = row['sleepData']\n",
    "  meanSleep = np.mean(sleepData)\n",
    "  varSleep = np.var(sleepData)\n",
    "  minSleep = np.min(sleepData)\n",
    "  maxSleep = np.max(sleepData)\n",
    "  lastSleep = sleepData[-1]\n",
    "\n",
    "  email = row['email']\n",
    "  curr_participant = participantInfo[participantInfo['Username'] == email].iloc[0]\n",
    "  bday = curr_participant['Birthday']\n",
    "  \n",
    "  try:\n",
    "    age = (datetime.datetime.now() - datetime.datetime.strptime(bday, \"%Y-%m-%d\")).days / 365\n",
    "  except Exception as e:\n",
    "    print(email, bday)\n",
    "    age = 26\n",
    "    \n",
    "  gender = curr_participant['Gender'] # CATEGORICAL !\n",
    "  trainingLevel = trainingDict[curr_participant['Level of Schooling']]\n",
    "\n",
    "  recall_scores = []\n",
    "  recog_scores = []\n",
    "  # 1st post-test\n",
    "  for y in ['A', 'B']:\n",
    "    for x in row['first_posttest'][y]:\n",
    "      if y == 'A':\n",
    "        score = x['automated_recall_score']\n",
    "        recall_scores.append(score)\n",
    "      else:\n",
    "        QID = x['QID']\n",
    "        score = 1 if x['response'] == correct_answers[QID] else 0\n",
    "        recog_scores.append(score)\n",
    "\n",
    "  test_time = 1\n",
    "  mat.append([\n",
    "    meanSleep,\n",
    "    varSleep,\n",
    "    minSleep,\n",
    "    maxSleep,\n",
    "    lastSleep,\n",
    "    age,\n",
    "    gender, \n",
    "    trainingLevel,\n",
    "    test_time,\n",
    "    np.mean(recall_scores),\n",
    "    np.mean(recog_scores)\n",
    "  ])\n",
    "\n",
    "  \n",
    "  recall_scores = []\n",
    "  recog_scores = []\n",
    "  # 2nd post-test\n",
    "  for y in ['A', 'B']:\n",
    "    for x in row['second_posttest'][y]:\n",
    "      if y == 'A':\n",
    "        score = x['automated_recall_score']\n",
    "        recall_scores.append(score)\n",
    "      else:\n",
    "        QID = x['QID']\n",
    "        score = 1 if x['response'] == correct_answers[QID] else 0\n",
    "        recog_scores.append(score)\n",
    "\n",
    "  test_time = 7\n",
    "  mat.append([\n",
    "    meanSleep,\n",
    "    varSleep,\n",
    "    minSleep,\n",
    "    maxSleep,\n",
    "    lastSleep,\n",
    "    age,\n",
    "    gender, \n",
    "    trainingLevel,\n",
    "    test_time,\n",
    "    np.mean(recall_scores),\n",
    "    np.mean(recog_scores)\n",
    "  ])\n",
    "\n",
    "\n",
    "  # 3rd post-test\n",
    "  recall_scores = []\n",
    "  recog_scores = []\n",
    "  for y in ['A', 'B']:\n",
    "    for _, x in row[f'posttest{y}'].iterrows():\n",
    "      if y == 'A':\n",
    "        score = x['automated_recall_score']\n",
    "        recall_scores.append(score)\n",
    "      else:\n",
    "        QID = x['QID']\n",
    "        score = 1 if x['response'] == correct_answers[QID] else 0\n",
    "        recog_scores.append(score)\n",
    "\n",
    "  test_time = 60\n",
    "  mat.append([\n",
    "    meanSleep,\n",
    "    varSleep,\n",
    "    minSleep,\n",
    "    maxSleep,\n",
    "    lastSleep,\n",
    "    age,\n",
    "    gender, \n",
    "    trainingLevel,\n",
    "    test_time,\n",
    "    np.mean(recall_scores),\n",
    "    np.mean(recog_scores)\n",
    "  ])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(mat, columns=['meanSleep', \"varSleep\", \"minSleep\", \"maxSleep\", \"lastSleep\",\n",
    "                                \"age\", \"gender\", \"trainingLevel\", \"test_time\", \"recallScore\", \"recogScore\"])\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    " \n",
    "print(\"RECALL\")\n",
    "lm = ols('recallScore ~ meanSleep + varSleep + minSleep + maxSleep + lastSleep + age + gender_Male + trainingLevel + test_time', df).fit()\n",
    "print(lm.summary())\n",
    "anova_table = anova_lm(lm)\n",
    "print('')\n",
    "print(anova_table['sum_sq'] / anova_table['sum_sq'].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RECOGNITION\")\n",
    "lm = ols('recogScore ~ meanSleep + varSleep + minSleep + maxSleep + lastSleep + age + gender_Male + trainingLevel + test_time', df).fit()\n",
    "print(lm.summary())\n",
    "anova_table = anova_lm(lm)\n",
    "print(anova_table['sum_sq'] / anova_table['sum_sq'].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter df to test_time = 1\n",
    "df1 = df[df['test_time'] == 1]\n",
    "\n",
    "# plot multiple scatterplots in one fig\n",
    "fig, axs = plt.subplots(1, 5, figsize=(25, 4))\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "for i, var in enumerate([\"meanSleep\", \"varSleep\", \"minSleep\", \"maxSleep\", \"lastSleep\"]):\n",
    "  axs[i].scatter(df1[var], df1['recallScore'])\n",
    "  axs[i].set_title(var)\n",
    "\n",
    "  # linear regression between maxSleep and score \n",
    "  x = df1[[var]]\n",
    "  y = df1['recallScore']\n",
    "  \n",
    "  x = sm.add_constant(x) # adding a constant\n",
    "  mod = sm.OLS(y, x)\n",
    "  fii = mod.fit()\n",
    "\n",
    "  # print R^2, coeff, and p-value from fii\n",
    "  print(f\"{var} | R^2: {fii.rsquared}, Coeff: {fii.params[1]}, p: {fii.pvalues[1]}\")\n",
    "\n",
    "  # plot line of best fit on meansleep plot\n",
    "  x = np.linspace(np.min(df1[var]), np.max(df1[var]), 100)\n",
    "  y = fii.params[0] + fii.params[1] * x\n",
    "  axs[i].plot(x, y, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter df to test_time = 1\n",
    "df1 = df[df['test_time'] == 1]\n",
    "df1\n",
    "\n",
    "# plot multiple scatterplots in one fig\n",
    "fig, axs = plt.subplots(1, 5, figsize=(25, 4))\n",
    "\n",
    "for i, var in enumerate([\"meanSleep\", \"varSleep\", \"minSleep\", \"maxSleep\", \"lastSleep\"]):\n",
    "  axs[i].scatter(df1[['trainingLevel']], df1[[var]])\n",
    "  axs[i].set_title(var)\n",
    "\n",
    "  # linear regression between maxSleep and score \n",
    "  x = df1[['trainingLevel']]\n",
    "  y = df1[[var]]\n",
    "\n",
    "  x = sm.add_constant(x) # adding a constant\n",
    "  mod = sm.OLS(y, x)\n",
    "  fii = mod.fit()\n",
    "\n",
    "  # print R^2, coeff, and p-value from fii\n",
    "  print(f\"{var} | R^2: {fii.rsquared}, Coeff: {fii.params[1]}, p: {fii.pvalues[1]}\")\n",
    "\n",
    "  # plot line of best fit on meansleep plot\n",
    "  x = np.linspace(np.min(df1['trainingLevel']), np.max(df1['trainingLevel']), 100)\n",
    "  y = fii.params[0] + fii.params[1] * x\n",
    "  axs[i].plot(x, y, color='red')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# create pandas df from CSV file\n",
    "df = pd.read_csv('/home/arpan/SAIL/paper/SAIL-Post-Survey.csv')\n",
    "surveyQuestions = df.iloc[0]\n",
    "surveyQuestions = surveyQuestions.to_dict()\n",
    "\n",
    "# drop the 1st row from df\n",
    "df = df.iloc[1:]\n",
    "\n",
    "df = df[~df[\"2\"].isin(bad_emails)]\n",
    "emails = df[\"2\"].unique()\n",
    "print(emails, \"num responses:\", len(emails))\n",
    "\n",
    "arr = [str(x) for x in range(12, 30)]\n",
    "arr.append(\"35\")\n",
    "arr.append(\"3\")\n",
    "df[arr] = df[arr].apply(pd.to_numeric)\n",
    "surveyQuestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4), dpi=300)\n",
    "bar_width = 0.2\n",
    "ax.set_ylim(-1, 13)\n",
    "# ax.bar(np.arange(4) - bar_width,\n",
    "#        [np.mean(df[\"12\"]), np.mean(df[\"13\"]), np.mean(df[\"14\"]), np.mean(df[\"15\"])], width=bar_width, label='Oral', color='#1b9e77', align=\"center\")\n",
    "# ax.bar(np.arange(4),\n",
    "#        [np.mean(df[\"16\"]), np.mean(df[\"17\"]), np.mean(df[\"18\"]), np.mean(df[\"19\"])], width=bar_width, label='Typed', color='#0295D9', align=\"center\")\n",
    "# ax.bar(np.arange(4) + bar_width,\n",
    "#        [np.mean(df[\"20\"]), np.mean(df[\"21\"]), np.mean(df[\"22\"]), np.mean(df[\"23\"])], width=bar_width, label='MC', color='#7570b3', align=\"center\")\n",
    "\n",
    "# plot box-and-whisker plot for each question\n",
    "c='#1b9e77'\n",
    "bp1 = ax.boxplot([df[\"12\"], df[\"13\"], df[\"14\"], df[\"15\"]], positions=np.arange(4) - bar_width - 0.05, widths=bar_width, showfliers=False,\n",
    "                 patch_artist=True, boxprops=dict(fc=c, color='black'), \n",
    "                 medianprops=dict(color='black'), flierprops=dict(color=c, markeredgecolor=c))\n",
    "\n",
    "c='#0295D9'\n",
    "bp2 = ax.boxplot([df[\"16\"], df[\"17\"], df[\"18\"], df[\"19\"]], positions=np.arange(4), widths=bar_width, showfliers=False,\n",
    "                 patch_artist=True, boxprops=dict(fc=c, color='black'), \n",
    "                 medianprops=dict(color='black'), flierprops=dict(color=c, markeredgecolor=c))\n",
    "\n",
    "c='#7570b3'\n",
    "bp3 = ax.boxplot([df[\"20\"], df[\"21\"], df[\"22\"], df[\"23\"]], positions=np.arange(4) + bar_width + 0.05, widths=bar_width, showfliers=False,\n",
    "                 patch_artist=True, boxprops=dict(fc=c, color='black'), \n",
    "                 medianprops=dict(color='black'), flierprops=dict(color=c, markeredgecolor=c))\n",
    "\n",
    "\n",
    "ax.set_xticks(np.arange(4),\n",
    "              ['Easy', 'Fun', \"Effective\", 'Efficient'], rotation=0, ha='center')\n",
    "\n",
    "\n",
    "ax.set_ylabel('Mean rating', labelpad=10)\n",
    "# ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=False, ncol=5)\n",
    "ax.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0], bp3[\"boxes\"][0]], ['Oral', 'Typed', 'MC'], \n",
    "          loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=False, ncol=5)\n",
    "\n",
    "# set y ticks\n",
    "ax.set_yticks(np.arange(0, 11, 2.5))\n",
    "\n",
    "# make font sizes way bigger\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.show()\n",
    "\n",
    "# save fig 600 dpi white bg\n",
    "fig.savefig('ratings.png', dpi=600, bbox_inches='tight', transparent=False, pad_inches=0.1, facecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA to compare modalities for each of these survey questions (Likert scale comparison)\n",
    "\n",
    "for i, var in enumerate([\"Easy\", \"Fun\", \"Effective\", \"Efficient\"]):\n",
    "  df1 = pd.DataFrame()\n",
    "  df1[\"rating\"] = df[f\"{12+i}\"]\n",
    "  df1[\"modality\"] = \"oral\"\n",
    "\n",
    "  df2 = pd.DataFrame()\n",
    "  df2[\"rating\"] = df[f\"{16+i}\"]\n",
    "  df2[\"modality\"] = \"typed\"\n",
    "\n",
    "  df3 = pd.DataFrame()\n",
    "  df3[\"rating\"] = df[f\"{20+i}\"]\n",
    "  df3[\"modality\"] = \"mc\"\n",
    "\n",
    "  df1 = pd.concat([df1, df2, df3])\n",
    "\n",
    "  # run a friedman to compare modalities\n",
    "  anova = friedmanchisquare(df1[df1['modality'] == \"oral\"]['rating'].to_numpy(), \n",
    "                            df1[df1['modality'] == \"typed\"]['rating'].to_numpy(), \n",
    "                            df1[df1['modality'] == \"mc\"]['rating'].to_numpy())\n",
    "  # print p-val\n",
    "  print(var, anova.pvalue)\n",
    "\n",
    "  # for modality in [\"oral\", \"typed\", \"mc\"]:\n",
    "  #   print(scipy.stats.shapiro(df1[df1['modality'] == modality]['rating'].to_numpy()))\n",
    "\n",
    "  # # # run turkey hsd to compare modalities\n",
    "  # # from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "  # # m_comp = pairwise_tukeyhsd(endog=df1['rating'], groups=df1['modality'], alpha=0.05)\n",
    "\n",
    "  # # print(m_comp)\n",
    "\n",
    "  # run a nonparametric repeated measures test to compare modalities\n",
    "  stat, p = wilcoxon(df1[df1['modality'] == \"oral\"]['rating'].to_numpy(), \n",
    "                     df1[df1['modality'] == \"typed\"]['rating'].to_numpy(),\n",
    "                     correction=True)\n",
    "  print(f\"Oral != Typed: {p}, reject: {True if p < (0.05/3) else False}\")\n",
    "\n",
    "  stat, p = wilcoxon(df1[df1['modality'] == \"oral\"]['rating'].to_numpy(), \n",
    "                     df1[df1['modality'] == \"mc\"]['rating'].to_numpy(),\n",
    "                     correction=True)\n",
    "  print(f\"Oral != MC: {p}, reject: {True if p < (0.05/3) else False}\")\n",
    "\n",
    "  stat, p = wilcoxon(df1[df1['modality'] == \"typed\"]['rating'].to_numpy(), \n",
    "                     df1[df1['modality'] == \"mc\"]['rating'].to_numpy(),\n",
    "                     correction=True)\n",
    "  print(f\"Typed != MC: {p}, reject: {True if p < (0.05/3) else False}\")\n",
    "\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casual = []\n",
    "clinical = []\n",
    "exam = []\n",
    "\n",
    "for index, i in enumerate([7, 8, 9]):\n",
    "  qNum = str(i)\n",
    "\n",
    "  # in df[qNum], replace all instances of \"Multiple choice\" with \"MC\"\n",
    "  df[qNum] = df[qNum].replace(\"Multiple choice\", \"MC\")\n",
    "  df[qNum] = df[qNum].replace(\"Multiple Choice\", \"MC\")\n",
    "  df[qNum] = df[qNum].replace(\"SAIL study app with voice\", \"Oral\")\n",
    "  df[qNum] = df[qNum].replace(\"Written response\", \"Typed\")\n",
    "  df[qNum] = df[qNum].replace(\"Written/typed response\", \"Typed\")\n",
    "\n",
    "  counts = []\n",
    "  labels = [\"Oral\", \"Typed\", \"MC\"]\n",
    "  for modality in labels:\n",
    "    if modality in df[qNum].value_counts():\n",
    "      counts.append(df[qNum].value_counts()[modality])\n",
    "    else:\n",
    "      counts.append(0)\n",
    "  \n",
    "  # change counts to a percentage of the sum of counts\n",
    "  original_counts = counts\n",
    "  counts = [x / sum(counts) for x in counts]\n",
    "  if i == 7:\n",
    "    casual = counts\n",
    "    casual_original = original_counts\n",
    "  elif i == 8:\n",
    "    clinical = counts\n",
    "    clinical_original = original_counts\n",
    "  elif i == 9:\n",
    "    exam = counts\n",
    "    exam_original = original_counts\n",
    "\n",
    "print(casual, clinical, exam)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4), dpi=600)\n",
    "bar_width = 0.2\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "ax.bar(np.arange(3) - bar_width,\n",
    "       [x[0] for x in [casual, clinical, exam]], width=bar_width, label='Oral', color='#1b9e77', align=\"center\", ec='black')\n",
    "ax.bar(np.arange(3),\n",
    "       [x[1] for x in [casual, clinical, exam]], width=bar_width, label='Typed', color='#0295D9', align=\"center\", ec='black')\n",
    "ax.bar(np.arange(3) + bar_width,\n",
    "       [x[2] for x in [casual, clinical, exam]], width=bar_width, label='MC', color='#7570b3', align=\"center\", ec='black')\n",
    "\n",
    "ax.set_xticks(np.arange(3),\n",
    "              ['Casual Learning', 'Clinical Prep', \"Exam Prep\"], rotation=0, ha='center')\n",
    "\n",
    "\n",
    "ax.set_ylabel('Proportion', labelpad=10)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=False, ncol=5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('proportions_per_setting.png', dpi=600, bbox_inches='tight', transparent=False, pad_inches=0.1, facecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare proportions\n",
    "\n",
    "for var in [\"Casual\", \"Clinical Prep\", \"Exam Prep\"]:\n",
    "  \n",
    "  metric = None\n",
    "  if var == \"Casual\":\n",
    "    metric = casual_original\n",
    "  elif var == \"Clinical Prep\":\n",
    "    metric = clinical_original\n",
    "  elif var == \"Exam Prep\":\n",
    "    metric = exam_original\n",
    "  \n",
    "  # chi square\n",
    "  f_obs = metric # [oral, typed, mc]\n",
    "  chi2, p = scipy.stats.chisquare(f_obs)\n",
    "  print(f\"{var} --> chi-square p: {p}\")\n",
    "\n",
    "  if var == 'Clinical Prep':\n",
    "    print('')\n",
    "    continue\n",
    "\n",
    "  # posthoc\n",
    "  for i, modality in enumerate([\"Oral\", \"Typed\", \"MC\"]):\n",
    "    # drop i from f_exp\n",
    "    f_obs = [x for j, x in enumerate(metric) if j != i]\n",
    "    chi2, p = scipy.stats.chisquare(f_obs)\n",
    "\n",
    "    modalities = [x for j,x in enumerate([\"Oral\", \"Typed\", \"MC\"]) if j != i]\n",
    "    print(f\"{modalities} --> chi-square p: {p}, reject = {True if p < (0.05/3) else False}\")\n",
    "  \n",
    "\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which learning modality did you like/enjoy the most?\n",
    "df['6'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Would you use SAIL in addition to your current study methods?\n",
    "df['32'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a result of using SAIL written and voice modes, do you feel you gained knowledge that can help you score higher \n",
    "# on an orthopedics in-training exam or written boards exam?\n",
    "df['11'].value_counts(normalize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Free Response Grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('../backend/scoring'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import score\n",
    "\n",
    "scorer = score.new_scorer(root='../backend/scoring', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(finished_post_data)):\n",
    "    for posttest_iteration in ['first_posttest', 'second_posttest', \"posttestA\"]:\n",
    "        if posttest_iteration == \"posttestA\":\n",
    "            posttestA = finished_post_data.iloc[i][posttest_iteration]\n",
    "        else:\n",
    "            posttestA = finished_post_data.iloc[i][posttest_iteration]['A']\n",
    "        \n",
    "        # if posttest_iteration == \"posttestA\":\n",
    "        #     print(posttestA.iloc[0]['start'])\n",
    "        for j in range(len(posttestA)):\n",
    "            if posttest_iteration == \"posttestA\":\n",
    "                # print(posttestA.iloc[j])\n",
    "                # print(finished_post_data.iloc[i]['email'], posttest_iteration, posttestA.iloc[j]['QID'], posttestA.iloc[j][\"response\"])\n",
    "                posttestA.at[j, \"automated_recall_score\"] = scorer.score(posttestA.iloc[j]['QID'], posttestA.iloc[j][\"response\"])\n",
    "                pass\n",
    "            else: \n",
    "                # print(finished_post_data.iloc[i]['email'], posttest_iteration, posttestA[j]['QID'], posttestA[j][\"response\"])\n",
    "                posttestA[j][\"automated_recall_score\"] = scorer.score(posttestA[j]['QID'], posttestA[j][\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(finished_post_data[\"posttestA\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(finished_post_data[\"first_posttest\"][0][\"A\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for posttest_iteration in ['posttestA', 'first_posttest', 'second_posttest']:\n",
    "    print(f\"Scoring {posttest_iteration}\")\n",
    "    for email, posttest in tqdm.tqdm(list(zip(finished_post_data['email'], finished_post_data[posttest_iteration]))):\n",
    "        with open(f'../backend/users/{email}.yaml') as f:\n",
    "            user_data = yaml.safe_load(f)\n",
    "        \n",
    "        if posttest_iteration != \"posttestA\": \n",
    "            for q, posttest_q in zip(user_data[posttest_iteration][\"A\"], posttest[\"A\"]):\n",
    "                q['automated_recall_score'] = posttest_q[\"automated_recall_score\"]\n",
    "        else:\n",
    "            posttest_array = [posttest.iloc[i].to_dict() for i in range(posttest.shape[0])]\n",
    "            for q, posttest_q in zip(user_data[posttest_iteration], posttest_array):\n",
    "                q['automated_recall_score'] = int(posttest_q[\"automated_recall_score\"])\n",
    "        \n",
    "        with open(f'../backend/users/{email}.yaml', 'w') as f:\n",
    "            yaml.dump(user_data, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sleep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep=finished_post_data['sleepData'].to_numpy()\n",
    "sleep=np.concatenate(sleep)\n",
    "plt.hist(sleep)\n",
    "plt.show()\n",
    "\n",
    "print(\"Sleep\")\n",
    "print(f\"Min: {np.min(sleep)}\")\n",
    "print(f\"Median: {np.median(sleep)}\")\n",
    "print(f\"Max: {np.max(sleep)}\")\n",
    "print(f\"Mean: {np.mean(sleep)}\")\n",
    "print(f\"SEM: {scipy.stats.sem(sleep)}\")\n",
    "\n",
    "\n",
    "# average sleep hrs might be an important factor\n",
    "# but also variability in sleep for a person might be important\n",
    "# or min"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a8a45cbae980bda797f4c4d96a25cc23e529283e4e0fc1b6546e39186b18bc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
